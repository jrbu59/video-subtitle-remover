#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›¿ä»£å­—å¹•æ£€æµ‹æ–¹æ¡ˆ - å½“Gemini APIä¸å¯ç”¨æ—¶çš„è§£å†³æ–¹æ¡ˆ
"""

import os
import sys
import cv2
import numpy as np
import logging
from typing import List, Tuple, Dict, Optional
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from backend.api.models.timed_subtitle import TimedSubtitleRegion, TimedSubtitleAnalysis


class AlternativeSubtitleDetector:
    """æ›¿ä»£å­—å¹•æ£€æµ‹å™¨"""

    def __init__(self, video_path: str):
        self.video_path = video_path
        self.cap = cv2.VideoCapture(video_path)

        if not self.cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.fps = self.cap.get(cv2.CAP_PROP_FPS)
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))

        logger.info(f"è§†é¢‘ä¿¡æ¯: {self.width}x{self.height}, {self.total_frames}å¸§, {self.fps:.1f}fps")

    def method1_ocr_based_detection(self, sample_frames: int = 20) -> TimedSubtitleAnalysis:
        """æ–¹æ³•1: åŸºäºOCRçš„å­—å¹•æ£€æµ‹"""
        logger.info("æ–¹æ³•1: ä½¿ç”¨OCRæ£€æµ‹å­—å¹•åŒºåŸŸ")

        try:
            from paddleocr import PaddleOCR
            ocr = PaddleOCR(use_angle_cls=True, lang='ch', show_log=False)
        except ImportError:
            logger.error("PaddleOCRæœªå®‰è£…ï¼Œè·³è¿‡OCRæ–¹æ³•")
            return self._create_empty_analysis()

        # é‡‡æ ·å¸§
        frame_interval = max(1, self.total_frames // sample_frames)
        detected_regions = []

        for i in range(0, min(sample_frames * frame_interval, self.total_frames), frame_interval):
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = self.cap.read()

            if not ret:
                continue

            # OCRæ£€æµ‹
            result = ocr.ocr(frame, cls=True)

            if result and result[0]:
                for detection in result[0]:
                    # æå–åæ ‡å’Œæ–‡æœ¬
                    coords = detection[0]
                    text = detection[1][0] if detection[1] else ""
                    confidence = detection[1][1] if detection[1] else 0.0

                    if confidence > 0.5 and len(text.strip()) > 2:  # è¿‡æ»¤ä½è´¨é‡æ£€æµ‹
                        # è®¡ç®—è¾¹ç•Œæ¡†
                        x_coords = [coord[0] for coord in coords]
                        y_coords = [coord[1] for coord in coords]

                        x_min, x_max = int(min(x_coords)), int(max(x_coords))
                        y_min, y_max = int(min(y_coords)), int(max(y_coords))

                        # æ£€æŸ¥æ˜¯å¦åœ¨åº•éƒ¨å­—å¹•åŒºåŸŸï¼ˆåº•éƒ¨30%ï¼‰
                        if y_min > self.height * 0.7:
                            detected_regions.append({
                                'frame_no': i,
                                'x': x_min,
                                'y': y_min,
                                'width': x_max - x_min,
                                'height': y_max - y_min,
                                'text': text,
                                'confidence': confidence
                            })

        return self._analyze_ocr_results(detected_regions)

    def method2_edge_detection(self, sample_frames: int = 30) -> TimedSubtitleAnalysis:
        """æ–¹æ³•2: åŸºäºè¾¹ç¼˜æ£€æµ‹çš„å­—å¹•åŒºåŸŸè¯†åˆ«"""
        logger.info("æ–¹æ³•2: ä½¿ç”¨è¾¹ç¼˜æ£€æµ‹è¯†åˆ«å­—å¹•åŒºåŸŸ")

        frame_interval = max(1, self.total_frames // sample_frames)
        subtitle_regions = []

        for i in range(0, min(sample_frames * frame_interval, self.total_frames), frame_interval):
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = self.cap.read()

            if not ret:
                continue

            # åªåˆ†æåº•éƒ¨30%åŒºåŸŸ
            bottom_region = frame[int(self.height * 0.7):, :]

            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(bottom_region, cv2.COLOR_BGR2GRAY)

            # åº”ç”¨é«˜æ–¯æ¨¡ç³Š
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)

            # è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(blurred, 50, 150)

            # å½¢æ€å­¦æ“ä½œè¿æ¥æ–‡æœ¬
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20, 5))
            closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)

            # æŸ¥æ‰¾è½®å»“
            contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            for contour in contours:
                # è®¡ç®—è½®å»“é¢ç§¯å’Œè¾¹ç•Œæ¡†
                area = cv2.contourArea(contour)
                if area > 1000:  # è¿‡æ»¤å°åŒºåŸŸ
                    x, y, w, h = cv2.boundingRect(contour)

                    # è°ƒæ•´åæ ‡åˆ°åŸå›¾
                    actual_y = y + int(self.height * 0.7)

                    # æ£€æŸ¥å®½é«˜æ¯”ï¼ˆå­—å¹•é€šå¸¸æ¯”è¾ƒå®½ï¼‰
                    aspect_ratio = w / h if h > 0 else 0
                    if aspect_ratio > 3 and w > self.width * 0.3:  # å®½åº¦è‡³å°‘30%
                        subtitle_regions.append({
                            'frame_no': i,
                            'x': x,
                            'y': actual_y,
                            'width': w,
                            'height': h,
                            'area': area,
                            'aspect_ratio': aspect_ratio
                        })

        return self._analyze_edge_results(subtitle_regions)

    def method3_template_matching(self) -> TimedSubtitleAnalysis:
        """æ–¹æ³•3: åŸºäºæ¨¡æ¿åŒ¹é…çš„æ£€æµ‹ï¼ˆéœ€è¦ç”¨æˆ·æä¾›å­—å¹•æ ·æœ¬ï¼‰"""
        logger.info("æ–¹æ³•3: åŸºäºæ¨¡æ¿åŒ¹é…æ£€æµ‹å­—å¹•åŒºåŸŸ")

        # è¿™é‡Œå¯ä»¥å®ç°åŸºäºå·²çŸ¥å­—å¹•æ¨¡æ¿çš„åŒ¹é…
        # ç”±äºæ²¡æœ‰é¢„å®šä¹‰æ¨¡æ¿ï¼Œè¿”å›åŸºäºç»éªŒçš„å›ºå®šåŒºåŸŸ

        return self._create_default_analysis()

    def method4_interactive_selection(self) -> TimedSubtitleAnalysis:
        """æ–¹æ³•4: äº¤äº’å¼åŒºåŸŸé€‰æ‹©ï¼ˆç”¨æˆ·æ‰‹åŠ¨æ ‡è®°ï¼‰"""
        logger.info("æ–¹æ³•4: äº¤äº’å¼åŒºåŸŸé€‰æ‹©")

        # æå–å‡ ä¸ªä»£è¡¨æ€§å¸§è®©ç”¨æˆ·é€‰æ‹©
        sample_frames = [
            int(self.total_frames * 0.1),  # 10%
            int(self.total_frames * 0.3),  # 30%
            int(self.total_frames * 0.7),  # 70%
            int(self.total_frames * 0.9),  # 90%
        ]

        print("\nè¯·æŸ¥çœ‹æå–çš„æ ·æœ¬å¸§ï¼Œç„¶åè¾“å…¥å­—å¹•åŒºåŸŸåæ ‡:")

        for i, frame_no in enumerate(sample_frames):
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)
            ret, frame = self.cap.read()

            if ret:
                # ä¿å­˜æ ·æœ¬å¸§
                sample_path = f"sample_frame_{i+1}_{frame_no}.jpg"
                cv2.imwrite(sample_path, frame)
                print(f"æ ·æœ¬å¸§ {i+1} å·²ä¿å­˜: {sample_path} (ç¬¬{frame_no}å¸§, {frame_no/self.fps:.1f}s)")

        print(f"\nè§†é¢‘å°ºå¯¸: {self.width}x{self.height}")
        print("è¯·æŸ¥çœ‹æ ·æœ¬å¸§ï¼Œç„¶åè¾“å…¥å­—å¹•åŒºåŸŸåæ ‡:")
        print("æ ¼å¼: x,y,width,height (ä¾‹å¦‚: 100,1500,880,150)")

        # åœ¨å®é™…ä½¿ç”¨ä¸­å¯ä»¥æ·»åŠ äº¤äº’è¾“å…¥
        # è¿™é‡Œæä¾›é»˜è®¤å€¼
        default_region = self._create_default_analysis()
        print("ä½¿ç”¨é»˜è®¤å­—å¹•åŒºåŸŸ...")

        return default_region

    def method5_motion_analysis(self, sample_frames: int = 50) -> TimedSubtitleAnalysis:
        """æ–¹æ³•5: åŸºäºè¿åŠ¨åˆ†æçš„å­—å¹•æ£€æµ‹"""
        logger.info("æ–¹æ³•5: åŸºäºè¿åŠ¨åˆ†ææ£€æµ‹å­—å¹•åŒºåŸŸ")

        frame_interval = max(1, self.total_frames // sample_frames)
        prev_frame = None
        subtitle_changes = []

        for i in range(0, min(sample_frames * frame_interval, self.total_frames), frame_interval):
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = self.cap.read()

            if not ret:
                continue

            # åªåˆ†æåº•éƒ¨åŒºåŸŸ
            bottom_region = frame[int(self.height * 0.75):, :]
            gray = cv2.cvtColor(bottom_region, cv2.COLOR_BGR2GRAY)

            if prev_frame is not None:
                # è®¡ç®—å¸§å·®
                diff = cv2.absdiff(prev_frame, gray)

                # åº”ç”¨é˜ˆå€¼
                _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)

                # è®¡ç®—å˜åŒ–åŒºåŸŸ
                change_ratio = np.sum(thresh > 0) / (thresh.shape[0] * thresh.shape[1])

                # å¦‚æœå˜åŒ–è¶…è¿‡é˜ˆå€¼ï¼Œå¯èƒ½æ˜¯å­—å¹•å˜åŒ–
                if change_ratio > 0.1:  # 10%å˜åŒ–
                    subtitle_changes.append({
                        'frame_no': i,
                        'change_ratio': change_ratio,
                        'prev_frame_no': i - frame_interval
                    })

            prev_frame = gray.copy()

        return self._analyze_motion_results(subtitle_changes)

    def _analyze_ocr_results(self, regions: List[Dict]) -> TimedSubtitleAnalysis:
        """åˆ†æOCRç»“æœ"""
        if not regions:
            return self._create_empty_analysis()

        # èšç±»ç›¸ä¼¼åŒºåŸŸ
        clustered_regions = self._cluster_regions(regions)

        timed_regions = []
        for cluster in clustered_regions:
            # ä¸ºæ¯ä¸ªèšç±»åˆ›å»ºæ—¶é—´æ®µ
            frames = [r['frame_no'] for r in cluster]
            start_frame = min(frames)
            end_frame = max(frames)

            # è®¡ç®—å¹³å‡åŒºåŸŸ
            avg_x = int(np.mean([r['x'] for r in cluster]))
            avg_y = int(np.mean([r['y'] for r in cluster]))
            avg_w = int(np.mean([r['width'] for r in cluster]))
            avg_h = int(np.mean([r['height'] for r in cluster]))
            avg_conf = np.mean([r['confidence'] for r in cluster])

            # æ‰©å±•æ—¶é—´èŒƒå›´ï¼ˆå‡è®¾å­—å¹•æŒç»­2-3ç§’ï¼‰
            duration_frames = int(self.fps * 2.5)
            expanded_start = max(0, start_frame - duration_frames // 2)
            expanded_end = min(self.total_frames - 1, end_frame + duration_frames // 2)

            region = TimedSubtitleRegion(
                start_frame=expanded_start,
                end_frame=expanded_end,
                x=avg_x,
                y=avg_y,
                width=avg_w,
                height=avg_h,
                confidence=avg_conf,
                text_content=f"OCRæ£€æµ‹åŒºåŸŸ (å¸§{start_frame}-{end_frame})"
            )
            timed_regions.append(region)

        return TimedSubtitleAnalysis(
            has_subtitles=len(timed_regions) > 0,
            subtitle_type="hard",
            timed_regions=timed_regions,
            total_frames=self.total_frames,
            fps=self.fps
        )

    def _analyze_edge_results(self, regions: List[Dict]) -> TimedSubtitleAnalysis:
        """åˆ†æè¾¹ç¼˜æ£€æµ‹ç»“æœ"""
        if not regions:
            return self._create_empty_analysis()

        # æŒ‰æ—¶é—´åˆ†ç»„
        time_groups = {}
        for region in regions:
            time_key = region['frame_no'] // int(self.fps * 2)  # æ¯2ç§’ä¸€ç»„
            if time_key not in time_groups:
                time_groups[time_key] = []
            time_groups[time_key].append(region)

        timed_regions = []
        for time_key, group in time_groups.items():
            if len(group) >= 2:  # è‡³å°‘è¦æœ‰2ä¸ªæ£€æµ‹ç‚¹
                frames = [r['frame_no'] for r in group]
                start_frame = min(frames)
                end_frame = max(frames)

                # è®¡ç®—ç»Ÿä¸€åŒºåŸŸ
                min_x = min([r['x'] for r in group])
                max_x = max([r['x'] + r['width'] for r in group])
                min_y = min([r['y'] for r in group])
                max_y = max([r['y'] + r['height'] for r in group])

                # æ‰©å±•æ—¶é—´èŒƒå›´
                duration_frames = int(self.fps * 3)
                expanded_start = max(0, start_frame - duration_frames // 2)
                expanded_end = min(self.total_frames - 1, end_frame + duration_frames // 2)

                region = TimedSubtitleRegion(
                    start_frame=expanded_start,
                    end_frame=expanded_end,
                    x=min_x,
                    y=min_y,
                    width=max_x - min_x,
                    height=max_y - min_y,
                    confidence=0.7,
                    text_content=f"è¾¹ç¼˜æ£€æµ‹åŒºåŸŸ (å¸§{start_frame}-{end_frame})"
                )
                timed_regions.append(region)

        return TimedSubtitleAnalysis(
            has_subtitles=len(timed_regions) > 0,
            subtitle_type="hard",
            timed_regions=timed_regions,
            total_frames=self.total_frames,
            fps=self.fps
        )

    def _analyze_motion_results(self, changes: List[Dict]) -> TimedSubtitleAnalysis:
        """åˆ†æè¿åŠ¨æ£€æµ‹ç»“æœ"""
        if not changes:
            return self._create_empty_analysis()

        # æ£€æµ‹å­—å¹•å˜åŒ–ç‚¹
        subtitle_segments = []
        current_start = None

        for change in changes:
            if current_start is None:
                current_start = change['frame_no']

            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­çš„å­—å¹•æ®µ
            if len(subtitle_segments) > 0:
                last_end = subtitle_segments[-1]['end']
                if change['frame_no'] - last_end > self.fps * 5:  # é—´éš”è¶…è¿‡5ç§’
                    # ç»“æŸå½“å‰æ®µï¼Œå¼€å§‹æ–°æ®µ
                    if current_start is not None:
                        subtitle_segments.append({
                            'start': current_start,
                            'end': change['frame_no'],
                            'changes': 1
                        })
                        current_start = change['frame_no']

        # å¤„ç†æœ€åä¸€æ®µ
        if current_start is not None and changes:
            subtitle_segments.append({
                'start': current_start,
                'end': changes[-1]['frame_no'],
                'changes': len(changes)
            })

        # åˆ›å»ºæ—¶é—´æ®µåŒºåŸŸ
        timed_regions = []
        for segment in subtitle_segments:
            if segment['changes'] >= 2:  # è‡³å°‘2æ¬¡å˜åŒ–
                region = TimedSubtitleRegion(
                    start_frame=segment['start'],
                    end_frame=segment['end'],
                    x=int(self.width * 0.1),
                    y=int(self.height * 0.8),
                    width=int(self.width * 0.8),
                    height=int(self.height * 0.15),
                    confidence=0.6,
                    text_content=f"è¿åŠ¨æ£€æµ‹åŒºåŸŸ (å¸§{segment['start']}-{segment['end']})"
                )
                timed_regions.append(region)

        return TimedSubtitleAnalysis(
            has_subtitles=len(timed_regions) > 0,
            subtitle_type="hard",
            timed_regions=timed_regions,
            total_frames=self.total_frames,
            fps=self.fps
        )

    def _cluster_regions(self, regions: List[Dict], distance_threshold: int = 50) -> List[List[Dict]]:
        """èšç±»ç›¸ä¼¼çš„å­—å¹•åŒºåŸŸ"""
        if not regions:
            return []

        clusters = []
        used = set()

        for i, region1 in enumerate(regions):
            if i in used:
                continue

            cluster = [region1]
            used.add(i)

            for j, region2 in enumerate(regions):
                if j in used:
                    continue

                # è®¡ç®—åŒºåŸŸç›¸ä¼¼åº¦
                x_diff = abs(region1['x'] - region2['x'])
                y_diff = abs(region1['y'] - region2['y'])
                w_diff = abs(region1['width'] - region2['width'])
                h_diff = abs(region1['height'] - region2['height'])

                if (x_diff < distance_threshold and
                    y_diff < distance_threshold and
                    w_diff < distance_threshold and
                    h_diff < distance_threshold):
                    cluster.append(region2)
                    used.add(j)

            if len(cluster) > 0:
                clusters.append(cluster)

        return clusters

    def _create_default_analysis(self) -> TimedSubtitleAnalysis:
        """åˆ›å»ºé»˜è®¤çš„å­—å¹•åˆ†æï¼ˆåŸºäºå¸¸è§å­—å¹•ä½ç½®ï¼‰"""
        # åˆ›å»ºä¸¤ä¸ªæ—¶é—´æ®µçš„é»˜è®¤å­—å¹•åŒºåŸŸ
        timed_regions = []

        # ç¬¬ä¸€æ®µ (0-40%è§†é¢‘æ—¶é•¿)
        if self.total_frames > 60:
            end_frame = min(int(self.total_frames * 0.4), self.total_frames - 1)
            region1 = TimedSubtitleRegion(
                start_frame=0,
                end_frame=end_frame,
                x=int(self.width * 0.05),
                y=int(self.height * 0.8),
                width=int(self.width * 0.9),
                height=int(self.height * 0.15),
                confidence=0.8,
                text_content="é»˜è®¤å­—å¹•åŒºåŸŸ1 (åº•éƒ¨)"
            )
            timed_regions.append(region1)

        # ç¬¬äºŒæ®µ (60%-90%è§†é¢‘æ—¶é•¿)
        if self.total_frames > 120:
            start_frame = int(self.total_frames * 0.6)
            end_frame = min(int(self.total_frames * 0.9), self.total_frames - 1)
            if start_frame < end_frame:
                region2 = TimedSubtitleRegion(
                    start_frame=start_frame,
                    end_frame=end_frame,
                    x=int(self.width * 0.05),
                    y=int(self.height * 0.8),
                    width=int(self.width * 0.9),
                    height=int(self.height * 0.15),
                    confidence=0.8,
                    text_content="é»˜è®¤å­—å¹•åŒºåŸŸ2 (åº•éƒ¨)"
                )
                timed_regions.append(region2)

        return TimedSubtitleAnalysis(
            has_subtitles=len(timed_regions) > 0,
            subtitle_type="hard",
            timed_regions=timed_regions,
            total_frames=self.total_frames,
            fps=self.fps
        )

    def _create_empty_analysis(self) -> TimedSubtitleAnalysis:
        """åˆ›å»ºç©ºçš„åˆ†æç»“æœ"""
        return TimedSubtitleAnalysis(
            has_subtitles=False,
            subtitle_type="hard",
            timed_regions=[],
            total_frames=self.total_frames,
            fps=self.fps
        )

    def run_all_methods(self) -> Dict[str, TimedSubtitleAnalysis]:
        """è¿è¡Œæ‰€æœ‰æ£€æµ‹æ–¹æ³•"""
        results = {}

        print(f"\n{'='*60}")
        print("è¿è¡Œæ‰€æœ‰å­—å¹•æ£€æµ‹æ–¹æ³•")
        print(f"{'='*60}")

        try:
            results['ocr'] = self.method1_ocr_based_detection()
            print("âœ… OCRæ£€æµ‹å®Œæˆ")
        except Exception as e:
            logger.error(f"OCRæ£€æµ‹å¤±è´¥: {e}")
            results['ocr'] = self._create_empty_analysis()

        try:
            results['edge'] = self.method2_edge_detection()
            print("âœ… è¾¹ç¼˜æ£€æµ‹å®Œæˆ")
        except Exception as e:
            logger.error(f"è¾¹ç¼˜æ£€æµ‹å¤±è´¥: {e}")
            results['edge'] = self._create_empty_analysis()

        try:
            results['motion'] = self.method5_motion_analysis()
            print("âœ… è¿åŠ¨åˆ†æå®Œæˆ")
        except Exception as e:
            logger.error(f"è¿åŠ¨åˆ†æå¤±è´¥: {e}")
            results['motion'] = self._create_empty_analysis()

        try:
            results['default'] = self._create_default_analysis()
            print("âœ… é»˜è®¤åŒºåŸŸç”Ÿæˆå®Œæˆ")
        except Exception as e:
            logger.error(f"é»˜è®¤åŒºåŸŸç”Ÿæˆå¤±è´¥: {e}")
            results['default'] = self._create_empty_analysis()

        return results

    def __del__(self):
        if hasattr(self, 'cap') and self.cap.isOpened():
            self.cap.release()


def main():
    """æµ‹è¯•æ›¿ä»£æ£€æµ‹æ–¹æ³•"""
    video_path = "/home/jiarui/software/video-subtitle-remover/videos/test_video_with_subtitle.mp4"

    if not os.path.exists(video_path):
        print(f"æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {video_path}")
        return

    detector = AlternativeSubtitleDetector(video_path)
    results = detector.run_all_methods()

    print(f"\n{'='*60}")
    print("æ£€æµ‹ç»“æœæ±‡æ€»")
    print(f"{'='*60}")

    for method, analysis in results.items():
        print(f"\nğŸ“‹ {method.upper()} æ–¹æ³•:")
        print(f"   æ£€æµ‹åˆ°å­—å¹•: {analysis.has_subtitles}")
        print(f"   æ—¶é—´æ®µæ•°é‡: {len(analysis.timed_regions)}")

        for i, region in enumerate(analysis.timed_regions):
            start_time = region.start_frame / analysis.fps
            end_time = region.end_frame / analysis.fps
            print(f"   åŒºåŸŸ{i+1}: {start_time:.1f}s-{end_time:.1f}s, "
                  f"ä½ç½®({region.x},{region.y}) å¤§å°{region.width}x{region.height}")

    # æ¨èæœ€ä½³æ–¹æ³•
    best_method = None
    max_regions = 0

    for method, analysis in results.items():
        if analysis.has_subtitles and len(analysis.timed_regions) > max_regions:
            max_regions = len(analysis.timed_regions)
            best_method = method

    if best_method:
        print(f"\nğŸ† æ¨èä½¿ç”¨: {best_method.upper()} æ–¹æ³• (æ£€æµ‹åˆ°{max_regions}ä¸ªæ—¶é—´æ®µ)")
    else:
        print(f"\nâš ï¸  æ‰€æœ‰æ–¹æ³•éƒ½æœªæ£€æµ‹åˆ°å­—å¹•ï¼Œå»ºè®®ä½¿ç”¨é»˜è®¤æ–¹æ³•")


if __name__ == "__main__":
    main()